dir_data <- "C:/Users/K.Lattery/SKIM/Philip Morris - F6886 Advanced modelling update Nov 21/3. Modelling/01 Magnit"
dir_work <- file.path(dir_data)
dir_base <- "C:/Users/K.Lattery/SKIM/Philip Morris - 3. Prework/3 final data"
data_tag <- file.path(dir_data, "Magnit_agg_wpromo_PanRussia.rds") # Pan Russia
data_full <- readRDS(data_tag) # Stacked data
check <- unique(data_full[,c(1,2)])
data_use <- data_full[data_full$dist> .05,]
codes_data <- sort(unique(data_use$code_variant))
model_pars <- readRDS(file.path(dir_work,"result_optim_att_sku_Magnit_3per_newprior4.rds"))
look <- model_pars$sku_to_attwcode
table(data_use$variable)
look <- unique(data_use[,c(3,5)])
View(model_pars)
betas <- model_pars$sku_betas_wcode
View(betas)
betas$b_trend[betas$code_variant == 10439] <- -.001
View(betas)
betas$b_trend[betas$code_variant == 10439] <- -.01
View(betas)
View(betas)
betas$b_trend[betas$code_variant == 10439] <- -.01123
betas$b_dist[betas$code_variant == 10439] <- 1
View(betas)
View(betas)
betas$b_trend[betas$code_variant == 11123] <- .01123
View(betas)
# Mariana Maroto, Modified to use Kevin's Github Jan 2022
# Google Analysis
# September 2019
# libraries
library(shiny)
library(shinycssloaders)
library(shinythemes)
library(dplyr)
library(reshape2)
library(tidyr)
library(data.table)
library(mclust)
# Use Kevin's Github code
source('https://raw.githubusercontent.com/klattery/RBUGS/main/RBUGS_Functions_v1.1.R')
source('https://raw.githubusercontent.com/klattery/MDwRate/main/MergeDesignwChoices.R')
source('https://raw.githubusercontent.com/klattery/MDwRate/main/RBUGS_Estimate_MDwAnchor.R')
# user interface
ui <- fluidPage(
shinyjs::useShinyjs(),
titlePanel(title=div(img(src="skim.png", height = 70), "Google Analysis Portal"),windowTitle = "Google Analysis Portal"),
tabsetPanel(
tabPanel("Data Transformation",
headerPanel(title = "Part 1: Merge and Transform Data"),
sidebarLayout(
sidebarPanel(
h3("Instructions:"),
h4("Upload cleaned Lighthouse Data and the design file (export it from Lighthouse) in the form of csv UTF-8. You can download the final files you will need to run the model."),
h4("Assumptions:"),
h4("1) Respondent Number should be the first column in your lighthouse data file."),
fileInput("data_file", placeholder = "Choose Lighthouse Data File (csv UTF-8)", multiple = FALSE, accept = c(
"text/csv","text/comma-separated-values,text/plain",".csv"), label = "Lighthouse Data File"),
fileInput("design_file", placeholder = "Choose Design File (csv UTF-8)", multiple = FALSE, accept = c(
"text/csv","text/comma-separated-values,text/plain",".csv"), label = "Design File"),
textInput("modulename", label = "Write MaxDiff Module Name:", value = "M"),
textInput("anchorname", label = "Write Anchor Question Name:", value = "Anchor"),
h4("2) If you Anchor Question is Binary, please recode your answer choices as 0 and 1. 1 Being the positive/yes answer and 0 being the negative/no answer."),
selectInput("anchortype", label = "Select Anchor Question Type:", choices = c("Point Scale","Binary")),
selectInput("scale", label = "Number of options in Point Scale Anchor Question:", choices = c(4,5,6,7,8,9)),
actionButton("setup_done", label = "Setup Done"),
h4("Download Files:"),
downloadButton("MDdatadownload","Max Diff Data Ready for Model"),
downloadButton("Ratedatadownload","Anchor Data Ready for Model")),
mainPanel(
h3("MD Data"),
dataTableOutput("MDdata_table"),
h3("Rate Data"),
dataTableOutput("Ratedata_table"))
)
),
tabPanel("Anchored Max Diff (RBUGS)",
headerPanel(title = "Part 2: Calculate Utilities using RBUGS for Anchored Max Diff"),
sidebarLayout(
sidebarPanel(
h3("Instructions:"),
h4("Click button 'Calculate' once you have checked your previous data tables in the last step. Might take around 30 minutes to compute."),
actionButton("calculate_rbugs", label = "Calculate"),
numericInput("nconceptscreen", label = "Specify number of concepts shown on screen to get the Exponentiated Utils.", value = 4),
h4("Download Files:"),
downloadButton("utilsdownload","Raw Utilities File"),
downloadButton("centutilsdownload","Centered Utilities File"),
downloadButton("exputilsdownload","Exponentiated Utilities File")
),
mainPanel(
h3("Raw Utilities"),
verbatimTextOutput("text_a"),
withSpinner(dataTableOutput("utils")),
h3("Centered Utilities (for LCA)"),
withSpinner(dataTableOutput("centutils")),
h3("Exponentiated Utilities (for SV)"),
withSpinner(dataTableOutput("exputils"))
)
)),
tabPanel("Other KPIs",
headerPanel(title = "Part 3: Calculate T2B KPI Scores:"),
sidebarLayout(
sidebarPanel(
numericInput("numkpis", label = "Enter number of KPIs questions:", value = 3),
uiOutput("writekpis"),
numericInput("scalekpis", label = "Enter number of response options for the KPIs questions:", value = 7),
actionButton("setup_done_2", label = "Calculate"),
h4("Download Files:"),
downloadButton("kpisdownload","Scores File")
),
mainPanel(
h3("Scores"),
dataTableOutput("kpitableshow")
))),
tabPanel("Clickables",
headerPanel(title = "Part 4: Calculate Clickables Counts:"),
sidebarLayout(
sidebarPanel(
h3("Instructions:"),
h4("Your lighthouse data file should contain column names named ex. 'C1b_part1item1' for every clickable question."),
textInput("posname", label = "Write name of positive clickable question:", value = "C1b"),
textInput("negname", label = "Write name of negative clickable question:", value = "C2b"),
actionButton("setup_done_3", label = "Get Counts"),
h4("Download Files:"),
downloadButton("posdownload","Positive Clickable Counts File"),
downloadButton("negdownload","Negative Clickable Counts File")
),
mainPanel(
h3("Positive Counts"),
dataTableOutput("poscountstable"),
h3("Negative Counts"),
dataTableOutput("negcountstable")
))),
tabPanel("LCA",
headerPanel(title = "Part 5: LCA"),
sidebarLayout(
sidebarPanel(
h3("Instructions:"),
h4("1. Upload centered utilities. Click 'Set up Done'."),
fileInput("new_centutils", placeholder = "Choose Centered Utils File (csv UTF-8)", multiple = FALSE, accept = c(
"text/csv","text/comma-separated-values,text/plain",".csv"), label = "Centered Utils File"),
actionButton("setup_done_4", label = "Set up Done"),
h4("2. Select the Model Name for LCA that works best for your data and works for creating 2,3,4, and 5 segments with your data."),
selectInput(inputId = "LCAmodelname", label = "Select Model", selected = "VEI",
choices = c("VEI","EEI","VVI","EVI","VVI","VVI","VII","EII","EEE","EVE","VEE","VVE","EEV","VEV","EVV","VVV")),
actionButton("model_selected", label = "Model Selected"),
h4("3. Download Files containing 4 columns, one with 2, 3, 4, and 5 segments each:"),
downloadButton("LCAsegmentsdown","LCA File")
),
mainPanel(
h3("LCA summary:"),
h3(verbatimTextOutput("LCAsummary")),
h3("Plot ranking LCA models:"),
plotOutput("LCAplot"),
h3("F Value Check on each Segmentation:"),
h3(textOutput("LCA_checks")),
h3("Segments Table:"),
dataTableOutput("LCAsegmentsshow")
)))
)
)
# server
server <- function(input, output) {
# helper functions from Kevin for LCA
fvalue_mult <- function(kdata, seg_def){
fvalue <- NULL
for (i in 1:ncol(seg_def)){
newdata <- cbind(seg_def[,i], kdata)
aov_k <- do.call(rbind,lapply(2:ncol(newdata), aov_result, col_group = 1, data_in = newdata))
fvalue <- cbind(fvalue, aov_k[,4])
}
colnames(fvalue) <- colnames(seg_def)
rownames(fvalue) <- colnames(kdata)
return(fvalue = round(fvalue,5))
}
aov_result <- function(col_test, col_group, data_in){
test <- aov(data_in[,col_test] ~ as.factor(data_in[,col_group]))
result <- summary(test)[[1]][1,]
return(result)
}
# read file from user
light_data <- reactive({read.csv(input$data_file$datapath)})
design_data <- reactive({read.csv(input$design_file$datapath)})
anchor_bounds <- reactive({
if (input$anchortype == "Binary") {
anchor <- "Binary"
} else {
anchor <- c(0.5, (as.numeric(input$scale) + 0.5))
}
anchor
})
output$writekpis <- renderUI({
writekpiname <- function (kpix){textInput(inputId = paste0("kpi",kpix),label = paste0("KPI ",kpix), value = paste0("A",kpix))}
lapply(1:input$numkpis, writekpiname)})
observeEvent(input$setup_done, {
# Apply function to conversion modules, plus combine all conversion modules
MDdata <- reactive({merge_function(light_data(), design_data(), input$modulename)})
Ratedata <- reactive({ratestack_function(light_data(), input$anchorname)})
output$MDdatadownload <- downloadHandler(
filename = "MD_stacked.csv", content = function (file) {write.csv(MDdata(),file, row.names = FALSE)})
output$Ratedatadownload <- downloadHandler(
filename = "Rate_stacked.csv", content = function (file) {write.csv(Ratedata(),file, row.names = FALSE)})
output$MDdata_table <- renderDataTable({MDdata()})
output$Ratedata_table <- renderDataTable({Ratedata()})
observeEvent(input$calculate_rbugs, {
utils <- reactive({
withCallingHandlers({
shinyjs::html("text_a", "")
rbugs_function(MDdata(), Ratedata(), anchor_bounds())
},
message = function(m) {
shinyjs::html(id = "text_a", html = m$message, add = TRUE)
})
})
centutils <- reactive({
raw_utils <- as.data.frame(utils())
raw_utils$k <- NULL
raw_utils$SynNone <- rowMeans(raw_utils[,2:(ncol(raw_utils))])
for (i in 2:(ncol(raw_utils)-1)) {
raw_utils[,i] <- raw_utils[,i]-raw_utils[,ncol(raw_utils)]
}
raw_utils
})
exputils <- reactive({
cent_utils <- as.data.frame(centutils())
for (i in 2:ncol(cent_utils)) {
cent_utils[,i] <- (exp(cent_utils[,i])/(exp(cent_utils[,i])+(input$nconceptscreen-1)))
}
cent_utils
})
output$utils <- renderDataTable({utils()})
output$utilsdownload <- downloadHandler(
filename = "export_utils.csv", content = function (file) {write.csv(utils(),file, row.names = FALSE, na = "")})
output$centutils <- renderDataTable({centutils()})
output$centutilsdownload <- downloadHandler(
filename = "export_centered_utils.csv", content = function (file) {write.csv(centutils(),file, row.names = FALSE, na = "")})
output$exputils <- renderDataTable({exputils()})
output$exputilsdownload <- downloadHandler(
filename = "export_exponentiated_utils.csv", content = function (file) {write.csv(exputils(),file, row.names = FALSE, na = "")})
})
})
observeEvent(input$setup_done_2, {
kpitable <- reactive({
master <- as.data.frame(light_data())
datalist = list()
datalist[[1]] = as.data.frame(master[,1])
for (i in 1:input$numkpis) {
kpi_data <-  master[,grep(paste0(input[[paste0("kpi",i)]],"_r*"), colnames( master))]
datalist[[i+1]] <- kpi_data
}
eval_table <- do.call(cbind, datalist)
colnames(eval_table)[1] <- "id_respondent"
eval_table <- melt(eval_table, id = "id_respondent")
eval_table <- separate(eval_table, variable, into = c("kpi", "concept_number"), sep = "_r")
eval_table$concept_number <- as.numeric(eval_table$concept_number)
eval_table$value[eval_table$value<(input$scalekpis-1)]  <- 0
eval_table$value[eval_table$value>(input$scalekpis-2)]  <- 1
eval_table <- as.data.table(eval_table)
eval_table_count0 <- as.data.frame(dcast.data.table(eval_table, concept_number ~ kpi, fun = length, subset = .(value == 0)))
eval_table_count1 <- as.data.frame(dcast.data.table(eval_table, concept_number ~ kpi, fun = length, subset = .(value == 1)))
for (j in 2:ncol(eval_table_count1)) {eval_table_count1[,j] <- (eval_table_count1[,j]/(eval_table_count1[,j]+eval_table_count0[,j]))}
eval_table_count1
})
output$kpitableshow <- renderDataTable({kpitable()})
output$kpisdownload <- downloadHandler(
filename = "kpi_t2b_scores.csv", content = function (file) {write.csv(kpitable(),file, row.names = FALSE, na = "")})
})
observeEvent(input$setup_done_3, {
clicktablepos <- reactive({
master <- as.data.frame(light_data())
pos_data <-  master[,grep(paste0(input$posname,"_*"), colnames( master))]
pos_data <- melt(pos_data)
pos_data <- separate(pos_data, variable, into = c("part", "item"), sep = "item")
pos_data$part <- gsub(pos_data$part, pattern = paste0(input$posname,"_part"), replacement="")
pos_data$part <- as.numeric(pos_data$part)
pos_data$item <- as.numeric(pos_data$item)
View(pos_data)
write.csv(pos_data, 'pos_data.csv')
sample_size <- dcast(pos_data[!is.na(pos_data$value),], part~item, fun = length, value.var = "value")
sample_size$part <- "total sample"
pos_data <- dcast(pos_data[!is.na(pos_data$value),], part~item, fun = sum, value.var = "value")
pos_data <- rbind(pos_data, sample_size[1,])
pos_data
})
clicktableneg <- reactive({
master <- as.data.frame(light_data())
neg_data <-  master[,grep(paste0(input$negname,"_*"), colnames( master))]
neg_data <- melt(neg_data)
neg_data <- separate(neg_data, variable, into = c("part", "item"), sep = "item")
neg_data$part <- gsub(neg_data$part, pattern = paste0(input$negname,"_part"), replacement="")
neg_data$part <- as.numeric(neg_data$part)
neg_data$item <- as.numeric(neg_data$item)
sample_size <- dcast(neg_data[!is.na(neg_data$value),], part~item, fun = length, value.var = "value")
neg_data <- dcast(neg_data[!is.na(neg_data$value),], part~item, fun = sum, value.var = "value")
sample_size$part <- "total sample"
neg_data <- rbind(neg_data, sample_size[1,])
neg_data
})
output$poscountstable <- renderDataTable({clicktablepos()})
output$negcountstable <- renderDataTable({clicktableneg()})
output$posdownload <- downloadHandler(
filename = "positive_click_counts.csv", content = function (file) {write.csv(clicktablepos(),file, row.names = FALSE, na = "")})
output$negdownload <- downloadHandler(
filename = "negative_click_counts.csv", content = function (file) {write.csv(clicktableneg(),file, row.names = FALSE, na = "")})
})
observeEvent(input$setup_done_4, {
# read file from user
betas <- reactive({
betas_in <- as.data.frame(read.csv(input$new_centutils$datapath))
betas <- betas_in[,-1]
})
BIC <- reactive({mclustBIC(betas(), G = 2:5)})
output$LCAplot <- renderPlot({plot(BIC())})
output$LCAsummary <- renderPrint({print(BIC())})
observeEvent(input$model_selected, {
LCA <- reactive({
LC_segs <- sapply(2:5, function(x) summary(BIC(), data = betas(), G = x, modelNames = input$LCAmodelname)$classification)
LC_segs <- as.data.frame(LC_segs)
LC_segs
})
output$LCA_checks <- renderText({
check <- fvalue_mult(betas(), as.data.frame(LCA()))
print(colMeans(check))
})
output$LCAsegmentsshow <- renderDataTable({LCA()})
output$LCAsegmentsdown <- downloadHandler(
filename = "LCA_segments.csv", content = function (file) {write.csv(LCA(),file, row.names = FALSE, na = "")})
})
})
}
shinyApp(ui,server)
indcode_spec_files
code_covariates
list_to_matrix
# libraries
library(shiny)
library(tidyverse)
library(dplyr)
library(reshape2)
library(shinycssloaders)
library(shinythemes)
library(DT)
library(tidyr)
source("https://raw.githubusercontent.com/klattery/Unspoken/master/Attraction_Conversion_Design_Generator_20SET20_kl3.R")
#source("https://raw.githubusercontent.com/klattery/Unspoken/master/UnspokenDesign_RShiny2.R")
#shinyApp(ui,server)
conversion_design <- conversion_function(
ntest = 80,
ntest_perver = 20,
ntest_comp = 0,
show_eachitem = 2,
items_task = 2,
n_versions = 30,
restrictions_table = NULL,
constraints_table = NULL,
shiny = FALSE
)
data_stack2 <- readRDS("C:/Users/K.Lattery/SKIM/Procter & Gamble - 4. Analysis/Stage3/Total/AWS_Code/Fac_Cov_Final/data_stack2.RDS")
dir <- "C:/Users/K.Lattery/SKIM/Procter & Gamble - 4. Analysis/Stage3/Total/AWS_Code/Fac_Cov_Final"
data_stack2 <- readRDS(file.path(dir, "data_stack2"))
data_stack2 <- readRDS(file.path(dir, "data_stack2.RDS"))
utilities_r <- read.csv(file.path(dir,"utilities_r_glue.csv"))
max_lev <- max(data_stack2$master_level)
nresp <- nrow(utilities_r)
util_recode_master <- matrix(0, nresp, max_lev)
link <- unique(data_stack2[,c(4,8)]) # The two master levels
View(link)
link[,1]
util_recode_master[,link[,1]] <- utilities_r[,link[,2]]
util_recode_master <- matrix(0, nresp, max_lev)
look <- util_recode_master[,link[,1]]
util_recode_master <- matrix(0, nresp, max_lev)
util_recode_master[,link[,1]] <- util_recode_master[,link[,1]] + utilities_r[,link[,2]]
View(utilities_r)
utilities_r_mat <- as.matrix(utilities_r[,-1])
util_recode_master <- matrix(0, nresp, max_lev)
util_recode_master[,link[,1]] <- util_recode_master[,link[,1]] + utilities_r[,link[,2]]
util_recode_master <- matrix(0, nresp, max_lev)
util_recode_master[,link[,1]] <- utilities_r_mat[,link[,2]]
View(util_recode_master)
util_sd <- apply(util_recode_master,2,sd)
util_sd
sum(util_sd < .00000001)
util_recode_master <- cbind(utilities_r[,1], util_recode_master)
write.csv(util_recode_master, file.path(dir, "utilities_glue_backcode.csv"), row.names = FALSE)
util_recode_master <- matrix(0, nresp, max_lev)
util_recode_master[,link[,1]] <- utilities_r_mat[,link[,2]]
util_sd <- apply(util_recode_master,2,sd)
sum(util_sd < .00000001)
util_recode_master <- cbind(id = utilities_r[,1], util_recode_master)
View(util_recode_master)
colnames(util_recode_master) <- paste0("claim_", 1:ncol(util_recode_master))
util_recode_master <- matrix(0, nresp, max_lev)
util_recode_master[,link[,1]] <- utilities_r_mat[,link[,2]]
colnames(util_recode_master) <- paste0("claim_", 1:ncol(util_recode_master))
util_sd <- apply(util_recode_master,2,sd)
sum(util_sd < .00000001)
util_recode_master <- cbind(id = utilities_r[,1], util_recode_master)
View(util_recode_master)
write.csv(util_recode_master, file.path(dir, "utilities_glue_backcode.csv"), row.names = FALSE)
install.packages("sqldf")
library(sqldf)
dir <- "C:/Users/K.Lattery/SKIM/Delta Airlines - Prelim Data for Kevin/Final/Data_Prep"
file_design <- "G1741_DelayCBC_Design for Kevin.csv"
file_data <- "G1741 Delay Model Prelim Data 9.30.csv"
file_out <- "G1741 Delay Prelim Recode NoFlip.csv"
# How to split compensation variable
#Each data set is different so this needs to be customized
comp_split <-  rbind(cbind(1:5,  1,0),
cbind(6:10, 2,5), # values 6-10, Type 2, Subtract 5,
cbind(11:15,3,10),
cbind(16:20,4,15),
cbind(21:25,5,20),
cbind(26:30,6,25))
design <- read.csv(file.path(dir,file_design))
data_resp <- read.csv(file.path(dir,file_data))
id_version <- data_resp[,c(1,2)]
colnames(design) <- c("version","task", "concept", "timing", "compensation", "severity")
colnames(id_version) <- c("id", "version")
choices <- data_resp[,seq(4,32, by = 2)]
ratings <- data_resp[,seq(5,33, by = 2)] # Raw data 1 = Definitely, 5 = Bad
answers_stack <- do.call(rbind,lapply(1:ncol(choices), function (i){
result <- cbind(id_version, task = i,  task_type = 1, choice = choices[,i], rating = ratings[,i])
return(result)
}))
# Now merge answers with design
data_stack <- sqldf("select * from answers_stack left join design using (version,task)
order by id, task, concept")
# Add conjoint response as 0/1
data_stack$dep <- as.numeric(data_stack$choice == data_stack$concept) # Code choice as 0/1
# Get ratings data and stack at end
data_stack2 <- data_stack[data_stack$dep == 1,] # Ratings of actual choices
data_stack2$task_type <- 2
data_stack2$dep <- data_stack2$rating
data_stack2$task <- 99
data_stack_all <- rbind(data_stack, data_stack2)
data_stack_all <- sqldf("select * from data_stack_all order by id, task, concept") # Sort
# Remove cols not needed
data_stack_all <- data_stack_all[,!names(data_stack_all) %in% c("version", "choice","rating")]
################################################
# Now decompose compensation attribute into type and level
# Use comp_split defined at top
ncols <- max(comp_split[,2])
comp_type <- comp_split[,2][data_stack_all$compensation] # Type of compensation
subtract <- comp_split[,3][data_stack_all$compensation] # Amount to Subtract to get level
comp_levels <- matrix(0, nrow = nrow(data_stack_all), ncol = ncols)
colnames(comp_levels) <- paste0("comp_level",1:ncols)
for (i in 1:ncols){
change_rows <- (comp_type == i)
comp_levels[change_rows, i] <- (data_stack_all$compensation - subtract)[change_rows]
}
data_stack_all <- cbind(data_stack_all, comp_type, comp_levels)
# Move dep to end
dep <- data_stack_all$dep # Copy
data_stack_all$dep <- NULL #Remove
data_stack_all$dep <- dep  # Copy to end
write.csv(data_stack_all, file.path(dir, file_out), row.names = FALSE)
library(sqldf)
dir <- "C:/Users/K.Lattery/SKIM/Delta Airlines - Kevin/Data_Prep"
file_design <- "G1741_DelayCBC_Design for Kevin.csv"
file_data <- "Delay_Final_10.7.csv"
file_out <- "G1741 Delay Final.csv"
design <- read.csv(file.path(dir,file_design))
data_resp <- read.csv(file.path(dir,file_data))
data_resp <- read.csv(file.path(dir,file_data))
data_resp <- read.csv(file.path(dir,file_data))
choices <- data_resp[,seq(3,31, by = 2)]
ratings <- data_resp[,seq(4,32, by = 2)] # Raw data 1 = Definitely, 5 = Bad
View(choices)
View(ratings)
design <- read.csv(file.path(dir,file_design))
id_version <- data_resp[,c(1,2)]
View(design)
colnames(design) <- c("version","task", "concept", "timing", "compensation", "severity")
colnames(id_version) <- c("id", "version")
answers_stack <- do.call(rbind,lapply(1:ncol(choices), function (i){
result <- cbind(id_version, task = i,  task_type = 1, choice = choices[,i], rating = ratings[,i])
return(result)
}))
# Now merge answers with design
data_stack <- sqldf("select * from answers_stack left join design using (version,task)
order by id, task, concept")
# Add conjoint response as 0/1
data_stack$dep <- as.numeric(data_stack$choice == data_stack$concept) # Code choice as 0/1
# Get ratings data and stack at end
data_stack2 <- data_stack[data_stack$dep == 1,] # Ratings of actual choices
data_stack2$task_type <- 2
data_stack2$dep <- data_stack2$rating
data_stack2$task <- 99
data_stack_all <- rbind(data_stack, data_stack2)
data_stack_all <- sqldf("select * from data_stack_all order by id, task, concept") # Sort
# Remove cols not needed
data_stack_all <- data_stack_all[,!names(data_stack_all) %in% c("version", "choice","rating")]
################################################
# Now decompose compensation attribute into type and level
# Use comp_split defined at top
ncols <- max(comp_split[,2])
comp_type <- comp_split[,2][data_stack_all$compensation] # Type of compensation
subtract <- comp_split[,3][data_stack_all$compensation] # Amount to Subtract to get level
comp_levels <- matrix(0, nrow = nrow(data_stack_all), ncol = ncols)
colnames(comp_levels) <- paste0("comp_level",1:ncols)
for (i in 1:ncols){
change_rows <- (comp_type == i)
comp_levels[change_rows, i] <- (data_stack_all$compensation - subtract)[change_rows]
}
data_stack_all <- cbind(data_stack_all, comp_type, comp_levels)
# Move dep to end
dep <- data_stack_all$dep # Copy
data_stack_all$dep <- NULL #Remove
data_stack_all$dep <- dep  # Copy to end
write.csv(data_stack_all, file.path(dir, file_out), row.names = FALSE)
file_out <- "G1741 Delay Final Stacked.csv"
write.csv(data_stack_all, file.path(dir, file_out), row.names = FALSE)
