#        type = "l" , lty = 1, lwd = 1, main = paste0("Chain ", chain_i), xlab = "Iteration", ylab = "Mean Beta")
}
pdf(file = file.path(dir_work, pdf_name),   # The directory you want to save the file in
width = 7, # The width of the plot in inches
height = 5) # The height of the plot in inches
hist(do.call(rbind,draws_beta$post_warmup_sampler_diagnostics)$accept_stat__, breaks = 30, main = "Acceptance Rate - Sampling", xlab = "", xlim = c(0,1))
for (chain_i in (1:nchains)){
matplot(1:nrow(draws_beta_mu[[chain_i]]), draws_beta_mu[[chain_i]],
type = "l" , lty = 1, lwd = 1, main = paste0("Chain ", chain_i), xlab = "Iteration", ylab = "Mean Beta")
}
chain_cols <- c("red","blue","green","black")
for (i in 1:ncol(draws_beta_mu[[1]])){
x <- sapply(1:length(draws_beta_mu), function(chain){
draws_beta_mu[[chain]][,i]
}) # x is set of column i across draws_beta_mu
fit_stats$mean[i] <- round(mean(x), 2)
fit_stats$rhat[i] <- round(rhat(x),2)
fit_stats$ESS[i] <- round(ess_basic(x),1)
plot(x[,1], type = "l", col = chain_cols[1], ylim = c(min(x), max(x)),
xlab = "Sample Iteration", ylab = "Mean Beta",
main = paste(colnames(code_master)[i],
"| rhat = ", round(rhat(x),2),
"| ESS = ", round(ess_basic(x),1)
))
for (chain in 2:nchains){
lines(x[,2], type = "l", col = chain_cols[chain])
}
}
dev.off()
write.table(fit_stats, file = file.path(dir_work, paste0(out_prefix,"_fit_stats.csv")), sep = ",", na = ".", row.names = FALSE)
}
checkconverge_export(stan_outname, data_stan$code_master, data_stan$I,
nchains = HB_fit$num_chains(), dir$stanout, out_prefix, dir$work)
checkconverge_export
source("https://raw.githubusercontent.com/klattery/Estimation-Ecosystem/master/EE_Functions_Stan2.R")
checkconverge_export(stan_outname, data_stan$code_master, data_stan$I,
nchains = HB_fit$num_chains(), dir$stanout, out_prefix, dir$work)
process_utilities(data_stan, utilities, out_prefix, dir$work)
write.table(cbind(data_stan$idtask, dep = data_stan$dep, wts = data_stan$wts, pred_all), file = file.path(dir_work, pred_name), sep = ",", na = ".", row.names = FALSE)
write.table(cbind(data_stan$idtask, dep = data_stan$dep, wts = data_stan$wts, pred = pred_all), file = file.path(dir_work, pred_name), sep = ",", na = ".", row.names = FALSE)
source("https://raw.githubusercontent.com/klattery/Estimation-Ecosystem/master/EE_Functions_Stan2.R")
source("https://raw.githubusercontent.com/klattery/Estimation-Ecosystem/master/EE_Functions_Stan2.R")
process_utilities(data_stan, utilities, out_prefix, dir$work)
process_utilities(data_stan, utilities, out_prefix, dir$work)
source("https://raw.githubusercontent.com/klattery/Estimation-Ecosystem/master/EE_Functions_Stan2.R")
process_utilities(data_stan, utilities, out_prefix, dir$work)
process_utilities
rm(env_eb)
rm(en_stan)
rm(env_stan)
em(env_code)
rm(env_code)
rm(env_modfun)
source("https://raw.githubusercontent.com/klattery/Estimation-Ecosystem/master/EE_Functions_Stan2.R")
process_utilities(data_stan, utilities, out_prefix, dir$work)
# 6) Optional Empirical Bayes =============
eb_betas_est(data_stan, draws_beta, colMeans(utilities), r_cores,
out_prefix, dir$work, cov_scale = 1, linux = FALSE)
source("https://raw.githubusercontent.com/klattery/Estimation-Ecosystem/master/EE_Functions_Stan2.R")
process_utilities(data_stan, utilities, out_prefix, dir$work)
# 6) Optional Empirical Bayes =============
eb_betas_est(data_stan, draws_beta, colMeans(utilities), r_cores,
out_prefix, dir$work, cov_scale = 1, linux = FALSE)
ind <- matrix(rnorm(670000*132), 670000, 132)
ksvd <- svd(cor(ind))
my_e <- eigen(cor(ind))
my_e[[1]]
ksvd$d
x <- cor(ind)
my_e <- eigen(x,only.values = TRUE)
my_e <- eigen(x,only.values = TRUE, symmetric = TRUE)
? cor
my_e <- eigen(ind,only.values = TRUE, symmetric = FALSE)
my_e <- eigen(ind %*% t(ind),only.values = TRUE, symmetric = FALSE)
my_e <- eigen(t(ind) %*% (ind),only.values = TRUE, symmetric = FALSE)
ind_z <- scale(ind)
my_e <- eigen(t(ind_z) %*% (ind_z),only.values = TRUE, symmetric = TRUE)
x <- cor(ind)
sessionInfo()
x <- cor(ind)
x2 <- crossprod(scale(ind, TRUE, TRUE))/(nrow(x)-1)
install.packages("coop")
library(coop)
coop::pcor(ind)
x2 <- coop::pcor(ind)
x2 <- crossprod(scale(ind, TRUE, TRUE))/(nrow(x)-1)
a <- Sys.time()
x2 <- crossprod(scale(ind, TRUE, TRUE))/(nrow(x)-1)
Sys.time()-a
a <- Sys.time()
x2 <- coop::pcor(ind)
Sys.time()-a
a <- Sys.time()
x <- cor(ind)
Sys.time()-a
? cor
x2 <- cor(ind, nThreads = 4)
sessionInfo()
a <- Sys.time()
x <- cor(ind)
Sys.time()-a # 7.23
a <- Sys.time()
x2 <- crossprod(scale(ind, TRUE, TRUE))/(nrow(x)-1)
Sys.time()-a #10.26
a <- Sys.time()
x2 <- coop::pcor(ind)
Sys.time()-a #4.73
plot(x,x2)
max(abs(x - x2))
# Uses model with price in population
library(readxl)
library(sqldf)
library(shiny)
dir_data <- "C:/Users/K.Lattery/SKIM/Philip Morris - F6886 Advanced modelling update Nov 21/3. Modelling/01 Magnit"
dir_work <- file.path(dir_data)
dir_base <- "C:/Users/K.Lattery/SKIM/Philip Morris - 3. Prework/3 final data"
data_tag <- file.path(dir_data, "Magnit_agg_wpromo_PanRussia.rds") # Pan Russia
data_full <- readRDS(data_tag) # Stacked data
check <- unique(data_full[,c(1,2)])
data_use <- data_full[data_full$dist> .05,]
codes_data <- sort(unique(data_use$code_variant))
model_pars <- readRDS(file.path(dir_work,"result_optim_att_sku_Magnit_3per_newprior4.rds"))
look <- model_pars$sku_to_attwcode
table(data_use$variable)
look <- unique(data_use[,c(3,5)])
View(model_pars)
betas <- model_pars$sku_betas_wcode
View(betas)
betas$b_trend[betas$code_variant == 10439] <- -.001
View(betas)
betas$b_trend[betas$code_variant == 10439] <- -.01
View(betas)
View(betas)
betas$b_trend[betas$code_variant == 10439] <- -.01123
betas$b_dist[betas$code_variant == 10439] <- 1
View(betas)
View(betas)
betas$b_trend[betas$code_variant == 11123] <- .01123
View(betas)
# Mariana Maroto, Modified to use Kevin's Github Jan 2022
# Google Analysis
# September 2019
# libraries
library(shiny)
library(shinycssloaders)
library(shinythemes)
library(dplyr)
library(reshape2)
library(tidyr)
library(data.table)
library(mclust)
# Use Kevin's Github code
source('https://raw.githubusercontent.com/klattery/RBUGS/main/RBUGS_Functions_v1.1.R')
source('https://raw.githubusercontent.com/klattery/MDwRate/main/MergeDesignwChoices.R')
source('https://raw.githubusercontent.com/klattery/MDwRate/main/RBUGS_Estimate_MDwAnchor.R')
# user interface
ui <- fluidPage(
shinyjs::useShinyjs(),
titlePanel(title=div(img(src="skim.png", height = 70), "Google Analysis Portal"),windowTitle = "Google Analysis Portal"),
tabsetPanel(
tabPanel("Data Transformation",
headerPanel(title = "Part 1: Merge and Transform Data"),
sidebarLayout(
sidebarPanel(
h3("Instructions:"),
h4("Upload cleaned Lighthouse Data and the design file (export it from Lighthouse) in the form of csv UTF-8. You can download the final files you will need to run the model."),
h4("Assumptions:"),
h4("1) Respondent Number should be the first column in your lighthouse data file."),
fileInput("data_file", placeholder = "Choose Lighthouse Data File (csv UTF-8)", multiple = FALSE, accept = c(
"text/csv","text/comma-separated-values,text/plain",".csv"), label = "Lighthouse Data File"),
fileInput("design_file", placeholder = "Choose Design File (csv UTF-8)", multiple = FALSE, accept = c(
"text/csv","text/comma-separated-values,text/plain",".csv"), label = "Design File"),
textInput("modulename", label = "Write MaxDiff Module Name:", value = "M"),
textInput("anchorname", label = "Write Anchor Question Name:", value = "Anchor"),
h4("2) If you Anchor Question is Binary, please recode your answer choices as 0 and 1. 1 Being the positive/yes answer and 0 being the negative/no answer."),
selectInput("anchortype", label = "Select Anchor Question Type:", choices = c("Point Scale","Binary")),
selectInput("scale", label = "Number of options in Point Scale Anchor Question:", choices = c(4,5,6,7,8,9)),
actionButton("setup_done", label = "Setup Done"),
h4("Download Files:"),
downloadButton("MDdatadownload","Max Diff Data Ready for Model"),
downloadButton("Ratedatadownload","Anchor Data Ready for Model")),
mainPanel(
h3("MD Data"),
dataTableOutput("MDdata_table"),
h3("Rate Data"),
dataTableOutput("Ratedata_table"))
)
),
tabPanel("Anchored Max Diff (RBUGS)",
headerPanel(title = "Part 2: Calculate Utilities using RBUGS for Anchored Max Diff"),
sidebarLayout(
sidebarPanel(
h3("Instructions:"),
h4("Click button 'Calculate' once you have checked your previous data tables in the last step. Might take around 30 minutes to compute."),
actionButton("calculate_rbugs", label = "Calculate"),
numericInput("nconceptscreen", label = "Specify number of concepts shown on screen to get the Exponentiated Utils.", value = 4),
h4("Download Files:"),
downloadButton("utilsdownload","Raw Utilities File"),
downloadButton("centutilsdownload","Centered Utilities File"),
downloadButton("exputilsdownload","Exponentiated Utilities File")
),
mainPanel(
h3("Raw Utilities"),
verbatimTextOutput("text_a"),
withSpinner(dataTableOutput("utils")),
h3("Centered Utilities (for LCA)"),
withSpinner(dataTableOutput("centutils")),
h3("Exponentiated Utilities (for SV)"),
withSpinner(dataTableOutput("exputils"))
)
)),
tabPanel("Other KPIs",
headerPanel(title = "Part 3: Calculate T2B KPI Scores:"),
sidebarLayout(
sidebarPanel(
numericInput("numkpis", label = "Enter number of KPIs questions:", value = 3),
uiOutput("writekpis"),
numericInput("scalekpis", label = "Enter number of response options for the KPIs questions:", value = 7),
actionButton("setup_done_2", label = "Calculate"),
h4("Download Files:"),
downloadButton("kpisdownload","Scores File")
),
mainPanel(
h3("Scores"),
dataTableOutput("kpitableshow")
))),
tabPanel("Clickables",
headerPanel(title = "Part 4: Calculate Clickables Counts:"),
sidebarLayout(
sidebarPanel(
h3("Instructions:"),
h4("Your lighthouse data file should contain column names named ex. 'C1b_part1item1' for every clickable question."),
textInput("posname", label = "Write name of positive clickable question:", value = "C1b"),
textInput("negname", label = "Write name of negative clickable question:", value = "C2b"),
actionButton("setup_done_3", label = "Get Counts"),
h4("Download Files:"),
downloadButton("posdownload","Positive Clickable Counts File"),
downloadButton("negdownload","Negative Clickable Counts File")
),
mainPanel(
h3("Positive Counts"),
dataTableOutput("poscountstable"),
h3("Negative Counts"),
dataTableOutput("negcountstable")
))),
tabPanel("LCA",
headerPanel(title = "Part 5: LCA"),
sidebarLayout(
sidebarPanel(
h3("Instructions:"),
h4("1. Upload centered utilities. Click 'Set up Done'."),
fileInput("new_centutils", placeholder = "Choose Centered Utils File (csv UTF-8)", multiple = FALSE, accept = c(
"text/csv","text/comma-separated-values,text/plain",".csv"), label = "Centered Utils File"),
actionButton("setup_done_4", label = "Set up Done"),
h4("2. Select the Model Name for LCA that works best for your data and works for creating 2,3,4, and 5 segments with your data."),
selectInput(inputId = "LCAmodelname", label = "Select Model", selected = "VEI",
choices = c("VEI","EEI","VVI","EVI","VVI","VVI","VII","EII","EEE","EVE","VEE","VVE","EEV","VEV","EVV","VVV")),
actionButton("model_selected", label = "Model Selected"),
h4("3. Download Files containing 4 columns, one with 2, 3, 4, and 5 segments each:"),
downloadButton("LCAsegmentsdown","LCA File")
),
mainPanel(
h3("LCA summary:"),
h3(verbatimTextOutput("LCAsummary")),
h3("Plot ranking LCA models:"),
plotOutput("LCAplot"),
h3("F Value Check on each Segmentation:"),
h3(textOutput("LCA_checks")),
h3("Segments Table:"),
dataTableOutput("LCAsegmentsshow")
)))
)
)
# server
server <- function(input, output) {
# helper functions from Kevin for LCA
fvalue_mult <- function(kdata, seg_def){
fvalue <- NULL
for (i in 1:ncol(seg_def)){
newdata <- cbind(seg_def[,i], kdata)
aov_k <- do.call(rbind,lapply(2:ncol(newdata), aov_result, col_group = 1, data_in = newdata))
fvalue <- cbind(fvalue, aov_k[,4])
}
colnames(fvalue) <- colnames(seg_def)
rownames(fvalue) <- colnames(kdata)
return(fvalue = round(fvalue,5))
}
aov_result <- function(col_test, col_group, data_in){
test <- aov(data_in[,col_test] ~ as.factor(data_in[,col_group]))
result <- summary(test)[[1]][1,]
return(result)
}
# read file from user
light_data <- reactive({read.csv(input$data_file$datapath)})
design_data <- reactive({read.csv(input$design_file$datapath)})
anchor_bounds <- reactive({
if (input$anchortype == "Binary") {
anchor <- "Binary"
} else {
anchor <- c(0.5, (as.numeric(input$scale) + 0.5))
}
anchor
})
output$writekpis <- renderUI({
writekpiname <- function (kpix){textInput(inputId = paste0("kpi",kpix),label = paste0("KPI ",kpix), value = paste0("A",kpix))}
lapply(1:input$numkpis, writekpiname)})
observeEvent(input$setup_done, {
# Apply function to conversion modules, plus combine all conversion modules
MDdata <- reactive({merge_function(light_data(), design_data(), input$modulename)})
Ratedata <- reactive({ratestack_function(light_data(), input$anchorname)})
output$MDdatadownload <- downloadHandler(
filename = "MD_stacked.csv", content = function (file) {write.csv(MDdata(),file, row.names = FALSE)})
output$Ratedatadownload <- downloadHandler(
filename = "Rate_stacked.csv", content = function (file) {write.csv(Ratedata(),file, row.names = FALSE)})
output$MDdata_table <- renderDataTable({MDdata()})
output$Ratedata_table <- renderDataTable({Ratedata()})
observeEvent(input$calculate_rbugs, {
utils <- reactive({
withCallingHandlers({
shinyjs::html("text_a", "")
rbugs_function(MDdata(), Ratedata(), anchor_bounds())
},
message = function(m) {
shinyjs::html(id = "text_a", html = m$message, add = TRUE)
})
})
centutils <- reactive({
raw_utils <- as.data.frame(utils())
raw_utils$k <- NULL
raw_utils$SynNone <- rowMeans(raw_utils[,2:(ncol(raw_utils))])
for (i in 2:(ncol(raw_utils)-1)) {
raw_utils[,i] <- raw_utils[,i]-raw_utils[,ncol(raw_utils)]
}
raw_utils
})
exputils <- reactive({
cent_utils <- as.data.frame(centutils())
for (i in 2:ncol(cent_utils)) {
cent_utils[,i] <- (exp(cent_utils[,i])/(exp(cent_utils[,i])+(input$nconceptscreen-1)))
}
cent_utils
})
output$utils <- renderDataTable({utils()})
output$utilsdownload <- downloadHandler(
filename = "export_utils.csv", content = function (file) {write.csv(utils(),file, row.names = FALSE, na = "")})
output$centutils <- renderDataTable({centutils()})
output$centutilsdownload <- downloadHandler(
filename = "export_centered_utils.csv", content = function (file) {write.csv(centutils(),file, row.names = FALSE, na = "")})
output$exputils <- renderDataTable({exputils()})
output$exputilsdownload <- downloadHandler(
filename = "export_exponentiated_utils.csv", content = function (file) {write.csv(exputils(),file, row.names = FALSE, na = "")})
})
})
observeEvent(input$setup_done_2, {
kpitable <- reactive({
master <- as.data.frame(light_data())
datalist = list()
datalist[[1]] = as.data.frame(master[,1])
for (i in 1:input$numkpis) {
kpi_data <-  master[,grep(paste0(input[[paste0("kpi",i)]],"_r*"), colnames( master))]
datalist[[i+1]] <- kpi_data
}
eval_table <- do.call(cbind, datalist)
colnames(eval_table)[1] <- "id_respondent"
eval_table <- melt(eval_table, id = "id_respondent")
eval_table <- separate(eval_table, variable, into = c("kpi", "concept_number"), sep = "_r")
eval_table$concept_number <- as.numeric(eval_table$concept_number)
eval_table$value[eval_table$value<(input$scalekpis-1)]  <- 0
eval_table$value[eval_table$value>(input$scalekpis-2)]  <- 1
eval_table <- as.data.table(eval_table)
eval_table_count0 <- as.data.frame(dcast.data.table(eval_table, concept_number ~ kpi, fun = length, subset = .(value == 0)))
eval_table_count1 <- as.data.frame(dcast.data.table(eval_table, concept_number ~ kpi, fun = length, subset = .(value == 1)))
for (j in 2:ncol(eval_table_count1)) {eval_table_count1[,j] <- (eval_table_count1[,j]/(eval_table_count1[,j]+eval_table_count0[,j]))}
eval_table_count1
})
output$kpitableshow <- renderDataTable({kpitable()})
output$kpisdownload <- downloadHandler(
filename = "kpi_t2b_scores.csv", content = function (file) {write.csv(kpitable(),file, row.names = FALSE, na = "")})
})
observeEvent(input$setup_done_3, {
clicktablepos <- reactive({
master <- as.data.frame(light_data())
pos_data <-  master[,grep(paste0(input$posname,"_*"), colnames( master))]
pos_data <- melt(pos_data)
pos_data <- separate(pos_data, variable, into = c("part", "item"), sep = "item")
pos_data$part <- gsub(pos_data$part, pattern = paste0(input$posname,"_part"), replacement="")
pos_data$part <- as.numeric(pos_data$part)
pos_data$item <- as.numeric(pos_data$item)
View(pos_data)
write.csv(pos_data, 'pos_data.csv')
sample_size <- dcast(pos_data[!is.na(pos_data$value),], part~item, fun = length, value.var = "value")
sample_size$part <- "total sample"
pos_data <- dcast(pos_data[!is.na(pos_data$value),], part~item, fun = sum, value.var = "value")
pos_data <- rbind(pos_data, sample_size[1,])
pos_data
})
clicktableneg <- reactive({
master <- as.data.frame(light_data())
neg_data <-  master[,grep(paste0(input$negname,"_*"), colnames( master))]
neg_data <- melt(neg_data)
neg_data <- separate(neg_data, variable, into = c("part", "item"), sep = "item")
neg_data$part <- gsub(neg_data$part, pattern = paste0(input$negname,"_part"), replacement="")
neg_data$part <- as.numeric(neg_data$part)
neg_data$item <- as.numeric(neg_data$item)
sample_size <- dcast(neg_data[!is.na(neg_data$value),], part~item, fun = length, value.var = "value")
neg_data <- dcast(neg_data[!is.na(neg_data$value),], part~item, fun = sum, value.var = "value")
sample_size$part <- "total sample"
neg_data <- rbind(neg_data, sample_size[1,])
neg_data
})
output$poscountstable <- renderDataTable({clicktablepos()})
output$negcountstable <- renderDataTable({clicktableneg()})
output$posdownload <- downloadHandler(
filename = "positive_click_counts.csv", content = function (file) {write.csv(clicktablepos(),file, row.names = FALSE, na = "")})
output$negdownload <- downloadHandler(
filename = "negative_click_counts.csv", content = function (file) {write.csv(clicktableneg(),file, row.names = FALSE, na = "")})
})
observeEvent(input$setup_done_4, {
# read file from user
betas <- reactive({
betas_in <- as.data.frame(read.csv(input$new_centutils$datapath))
betas <- betas_in[,-1]
})
BIC <- reactive({mclustBIC(betas(), G = 2:5)})
output$LCAplot <- renderPlot({plot(BIC())})
output$LCAsummary <- renderPrint({print(BIC())})
observeEvent(input$model_selected, {
LCA <- reactive({
LC_segs <- sapply(2:5, function(x) summary(BIC(), data = betas(), G = x, modelNames = input$LCAmodelname)$classification)
LC_segs <- as.data.frame(LC_segs)
LC_segs
})
output$LCA_checks <- renderText({
check <- fvalue_mult(betas(), as.data.frame(LCA()))
print(colMeans(check))
})
output$LCAsegmentsshow <- renderDataTable({LCA()})
output$LCAsegmentsdown <- downloadHandler(
filename = "LCA_segments.csv", content = function (file) {write.csv(LCA(),file, row.names = FALSE, na = "")})
})
})
}
shinyApp(ui,server)
indcode_spec_files
code_covariates
list_to_matrix
# libraries
library(shiny)
library(tidyverse)
library(dplyr)
library(reshape2)
library(shinycssloaders)
library(shinythemes)
library(DT)
library(tidyr)
source("https://raw.githubusercontent.com/klattery/Unspoken/master/Attraction_Conversion_Design_Generator_20SET20_kl3.R")
#source("https://raw.githubusercontent.com/klattery/Unspoken/master/UnspokenDesign_RShiny2.R")
#shinyApp(ui,server)
conversion_design <- conversion_function(
ntest = 80,
ntest_perver = 20,
ntest_comp = 0,
show_eachitem = 2,
items_task = 2,
n_versions = 30,
restrictions_table = NULL,
constraints_table = NULL,
shiny = FALSE
)
data_stack2 <- readRDS("C:/Users/K.Lattery/SKIM/Procter & Gamble - 4. Analysis/Stage3/Total/AWS_Code/Fac_Cov_Final/data_stack2.RDS")
dir <- "C:/Users/K.Lattery/SKIM/Procter & Gamble - 4. Analysis/Stage3/Total/AWS_Code/Fac_Cov_Final"
data_stack2 <- readRDS(file.path(dir, "data_stack2"))
data_stack2 <- readRDS(file.path(dir, "data_stack2.RDS"))
utilities_r <- read.csv(file.path(dir,"utilities_r_glue.csv"))
max_lev <- max(data_stack2$master_level)
nresp <- nrow(utilities_r)
util_recode_master <- matrix(0, nresp, max_lev)
link <- unique(data_stack2[,c(4,8)]) # The two master levels
View(link)
link[,1]
util_recode_master[,link[,1]] <- utilities_r[,link[,2]]
util_recode_master <- matrix(0, nresp, max_lev)
look <- util_recode_master[,link[,1]]
util_recode_master <- matrix(0, nresp, max_lev)
util_recode_master[,link[,1]] <- util_recode_master[,link[,1]] + utilities_r[,link[,2]]
View(utilities_r)
utilities_r_mat <- as.matrix(utilities_r[,-1])
util_recode_master <- matrix(0, nresp, max_lev)
util_recode_master[,link[,1]] <- util_recode_master[,link[,1]] + utilities_r[,link[,2]]
util_recode_master <- matrix(0, nresp, max_lev)
util_recode_master[,link[,1]] <- utilities_r_mat[,link[,2]]
View(util_recode_master)
util_sd <- apply(util_recode_master,2,sd)
util_sd
sum(util_sd < .00000001)
util_recode_master <- cbind(utilities_r[,1], util_recode_master)
write.csv(util_recode_master, file.path(dir, "utilities_glue_backcode.csv"), row.names = FALSE)
util_recode_master <- matrix(0, nresp, max_lev)
util_recode_master[,link[,1]] <- utilities_r_mat[,link[,2]]
util_sd <- apply(util_recode_master,2,sd)
sum(util_sd < .00000001)
util_recode_master <- cbind(id = utilities_r[,1], util_recode_master)
View(util_recode_master)
colnames(util_recode_master) <- paste0("claim_", 1:ncol(util_recode_master))
util_recode_master <- matrix(0, nresp, max_lev)
util_recode_master[,link[,1]] <- utilities_r_mat[,link[,2]]
colnames(util_recode_master) <- paste0("claim_", 1:ncol(util_recode_master))
util_sd <- apply(util_recode_master,2,sd)
sum(util_sd < .00000001)
util_recode_master <- cbind(id = utilities_r[,1], util_recode_master)
View(util_recode_master)
write.csv(util_recode_master, file.path(dir, "utilities_glue_backcode.csv"), row.names = FALSE)
